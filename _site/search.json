[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luis Paese",
    "section": "",
    "text": "Data Science meets Infrastructure as Code\n\n\n\n\n\n\n\n\n\nMar 17, 2021\n\n\nLuis HZ Paese\n\n\n0 min\n\n\n\n\n\n\n\n\nUsing GitHub Actions to speed up CI/CD in data science projects\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\nLuis HZ Paese\n\n\n11 min\n\n\n\n\n\n\n\n\nUsing RStudio Project Templates to help the project standardization in data science teams\n\n\nImproving team collaboration and project standardization in data science\n\n\n\n\n\n\nApr 29, 2020\n\n\nLuis HZ Paese\n\n\n0 min\n\n\n\n\n\n\nNo matching items\n\n\n See all posts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there, my name is Luis HZ Paese, and I’m a Machine Learning Engineer!"
  },
  {
    "objectID": "posts/teste1.html",
    "href": "posts/teste1.html",
    "title": "Teste1",
    "section": "",
    "text": "Teste1\nasdadasdasdad"
  },
  {
    "objectID": "posts/teste2.html",
    "href": "posts/teste2.html",
    "title": "Teste2",
    "section": "",
    "text": "Teste2\nasdadasdasdad"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nUsing GitHub Actions to speed up CI/CD in data science projects\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio Project Templates to help the project standardization in data science teams\n\n\nImproving team collaboration and project standardization in data science\n\n\n\nApr 29, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/teste2/index.html",
    "href": "posts/teste2/index.html",
    "title": "Teste2",
    "section": "",
    "text": "Teste2\nasdadasdasdad"
  },
  {
    "objectID": "posts/teste1/index.html",
    "href": "posts/teste1/index.html",
    "title": "Teste1",
    "section": "",
    "text": "Teste1\nasdadasdasdad"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Luis Paese",
    "section": "",
    "text": "Publications\n\n\nEarly childhood home-based programmes and school violence: evidence from Brazil\n\nWith MV Wink Junior and FG Ribeiro\nDevelopment in Practice 32 (2), 133-143\n\nImpacts of grade configuration on Brazilian student outcomes\n\nWith MV Wink Junior and MC Griebeler\nRevista Brasileira de Economia 75, 91-115\n\nLate and Unequal: Measuring Enrolments and Retention in Brazilian Education, 1933-2010\n\nWith TH Kang and NFA Felix\nRevista de Historia Economica-Journal of Iberian and Latin American Economic History, 1-28\n\nThe diversification benefits of cryptocurrencies in multi-asset portfolios: Cross-country evidence\n\nWith JA Colombo, FIL Cruz and RX Cortes\nAvailable at SSRN 3776260\n\nInequality of educational opportunities: Evidence from Brazil\n\nWith MV Wink Junior\nEconomiA 20 (2), 109-120\n\nAre Cryptocurrencies Suitable for Portfolio Diversification? Cross-Country Evidence\n\nWith JA Colombo, FIL Cruz and RX Cortes\n47º ENCONTRO NACIONAL DE ECONOMIA - ANPEC\n\nBuilding State-Level Business Cycle Tracer Tools: Evidence from a Large Emerging Economy\n\nWith JA Colombo, FIL Cruz and RX Cortes\nInternational Journal of Economics and Finance 10 (5), 14-30\n\nNível e desigualdade de aprendizado escolar: uma análise a partir dos desempenhos dos Coredes gaúchos no Sistema Nacional de Avaliação da Educação da Básica 2013 - In Portuguese\n\nWith MV Wink Junior\nIndicadores Econômicos FEE 44 (4), 43-52\n\n\n\n\nWorking Papers\n\n\nNo current Working Papers"
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html",
    "href": "posts/gh-actions-data-science/index.html",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "",
    "text": "As the latest advances regarding cloud computing, it has become even more necessary for the implementation of tools that are, at the same time, scalable, and that ensure the reproducibility of the execution. Having this need in mind, a few tools arose to the job, such as Docker, that allow the creation of a “recipe” of the application, ensuring that different builds of the same application run equally.\nDifferently than a Virtual Machine (VM), that provides an infrastructure through Hypervisors and emulates processors and memory, Docker shares these resources throughout the containers, allowing the developer to focus less on the infrastructure and more on the development of the application. Nonetheless, the containerization of projects and applications mitigates the expression “It Runs on My Machine”, given that it tries to ensure that, independently on the platform chosen by the developer, the Docker container executes always in the same way.\nGiven that the benefits of containerization go beyond the development of applications and can be useful in other fields, many data scientists began to use Docker to containerize their analysis, model training, dashboards, and APIs, both to make the delivery of projects easier (given that it reduces the possibility of bugs) and to ensure that the results found once, can always be reached again."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#cicd-continous-integration-and-continuous-deployment",
    "href": "posts/gh-actions-data-science/index.html#cicd-continous-integration-and-continuous-deployment",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "CI/CD — Continous Integration and Continuous Deployment",
    "text": "CI/CD — Continous Integration and Continuous Deployment\nEven though many insights and machine learning models generated by data scientists are valuable, they fail to add value to the business they are inserted when the projects are stuck in a personal machine that cannot be used by other people. Therefore, to ensure that any modifications are identified and its results are expanded to other teams, there is the process of Continous Integration and Continous Deployment (CI/CD), that allows the automation of the testing and deploy processes in initial versions of the project.\nMany people may be familiar with the concepts of CI/CD, however, many of the tools that are used for this process are paid (such as Jenkins, CircleCI, and TravisCI), limiting its use only to people that:\n\nare willing to pay the price of these tools;\nor are working in a company that already has a CI/CD cycle deployed."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#github-and-github-actions",
    "href": "posts/gh-actions-data-science/index.html#github-and-github-actions",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "GitHub and GitHub Actions",
    "text": "GitHub and GitHub Actions\nGitHub is a well-known code versioning platform that has more than 40 million users and more than 100 million repositories, being an enormous source of open-source code, available to thousands of people anywhere in the world.\nAiming to support the creation of open-source projects, as well as allow their users to abstract the processes involving CI/CD, GitHub created in 2019, the GitHub Actions tool. It allows the automation of workflows defined by the user to help with integrated tests, validation of Pull Requests, and many other features. Moreover, the number of actions to be used by the user grows by the day, given that many companies are looking to develop tools to help the users community. Many of these actions already allow the integration of many popular tools such as Docker, AWS CloudFormation, Terraform, and many others that can be found here.\nEven though GitHub Actions is free to use only for non-private repositories, there are different levels of use that can be leveraged in private projects, before considering the use of any GitHub Enterprise tools. This opens the doors so that many people that develop open-source projects can test their tools and spread their findings in a more automated and scalable way."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#docker-login-docker-build-and-push",
    "href": "posts/gh-actions-data-science/index.html#docker-login-docker-build-and-push",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "Docker Login & Docker Build and Push",
    "text": "Docker Login & Docker Build and Push\nOne of the tools developed to be used alongside GitHub Actions is the login actions in repositories that allow the storage of Docker images (such as Docker Hub, ECR from AWS, GCR from GCP), as well as the build of these images without the need to occupy the user’s machine. With these in mind, two actions are declared in the CI workflow file, that can be found in these links: docker/login-action and docker/build-push-action."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#inserting-data-science-in-the-cicd-process",
    "href": "posts/gh-actions-data-science/index.html#inserting-data-science-in-the-cicd-process",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "Inserting Data Science in the CI/CD process",
    "text": "Inserting Data Science in the CI/CD process\nThe Data Science field is full of different frameworks, dependencies, and different languages that can be used according to the need and abilities of the data scientist, but a common truth amongst them is that they all have the possibility of being encapsulated by a containerization process, helping to ensure the reproducibility of the project.\nWith that in mind, the example used by me to deploy the automation tool of GitHub Actions involves the development of a web application using R’s Shiny library. Nevertheless, the same workflow implementation could be used to deploy APIs developed using Python’s FastAPI, i.e., or any other framework that can be encapsulated in a Docker container.\nThe project can be found here: paeselhz/ghActionsDockerShiny. I won’t be entering in details of the development of the application, because the example used by me is relatively simple and have no elaborate development. The focus of this article is the containerization of the project, and the workflow automation to build the image and store it in Docker Hub, making it available for further downloads.\n\nCreating the Dockerfile\nFor those familiarized with the Dockerfile and its syntax, the execution is the same as expected in a project that will be developed, built, and ran locally with Docker. In it, we declare the base image that will be used for further installation of libraries and dependencies, as well as the configuration of the project, file copy, and other steps that usually can be added to a Dockerfile.\nFROM rocker/shiny:4.0.0\n\nRUN apt-get update \\\n  && apt-get install -y \\\n    libxml2-dev \\\n    libglpk-dev \\\n  && install2.r \\\n    --error \\\n    dplyr \\\n    shiny \\\n    purrr \\\n    highcharter \\\n    shinyWidgets \\\n    shinycssloaders \\\n    devtools \\\n    xml2 \\\n    igraph \\\n    readr\n  \nRUN R -e \"devtools::install_github('wilsonfreitas/rbcb')\"\nCOPY . /srv/shiny-server\nRUN chmod -R 777 /srv/shiny-server\nThis script, which is located in the project root directory, is responsible to gather an image that already has Shiny and its dependencies installed, and the installation of libraries that will be used by the app developed within R.\n\n\nCreating the Workflow file\nTo GitHub Actions know which steps need to be taken for the workflow automation, it becomes necessary to create a file within the project that will be located at .github/workflows/main.yml, the file syntax is the same as any YAML file, being easy to code. In case the user does not want to do this process locally and commit the changes, GitHub itself has an online code editor for the creation of the workflow.\nIn this file are declared a few steps such as the name of the workflow, the triggers that will be used to deploy the workflow execution, and the jobs that it will be responsible for executing. The name and trigger parts of the file are highly customizable, and the user can change it in many ways, moreover, in the part of the job, there are a few steps that are needed for the job to login in Docker Hub, configure BuildX (a tool that will be used to build the image), configure QEMU (a tool that will be allowing multi-platform builds), deploy the built image to Docker Hub, logout and clean the machine to ensure that no processes are still running.\n# Setting up a Workflow to work with Github Actions\nname: ci\n# Controls to when trigger the GH Action\n# Below are configurations to the following triggers:\n# - commits at master branch\n# - tag commits at the project\n# - scheduled to run at 01:00GMT\n# The user can also configure triggers at pull requests\n# as well as remove branches from triggering GH Actions\non:\n  push:\n    branches: [ master ]\n    tags: [ '*.*.*' ]\n  schedule:\n    - cron: '0 1 * * *'\n# Below there is the job configuration to build the image\n# and push it to a DockerHub repository\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Checkout\n        uses: actions/checkout@v2\n      -\n        name: Prepare\n        id: prep\n        run: |\n          DOCKER_IMAGE=<USER_NAME>/<REPOSITORY_NAME>\n          VERSION=noop\n          if [ \"${{ github.event_name }}\" = \"schedule\" ]; then\n            VERSION=nightly\n          elif [[ $GITHUB_REF == refs/tags/* ]]; then\n            VERSION=${GITHUB_REF#refs/tags/}\n          elif [[ $GITHUB_REF == refs/heads/* ]]; then\n            VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r 's#/+#-#g')\n            if [ \"${{ github.event.repository.default_branch }}\" = \"$VERSION\" ]; then\n              VERSION=edge\n            fi\n          fi\n          TAGS=\"${DOCKER_IMAGE}:${VERSION}\"\n          if [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n            MINOR=${VERSION%.*}\n            MAJOR=${MINOR%.*}\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:${MINOR},${DOCKER_IMAGE}:${MAJOR},${DOCKER_IMAGE}:latest\"\n          elif [ \"${{ github.event_name }}\" = \"push\" ]; then\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:sha-${GITHUB_SHA::8}\"\n          fi\n          echo ::set-output name=version::${VERSION}\n          echo ::set-output name=tags::${TAGS}\n          echo ::set-output name=created::$(date -u +'%Y-%m-%dT%H:%M:%SZ')\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v1\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v1\n      -\n        name: Login to DockerHub\n        if: github.event_name != 'pull_request'\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        id: docker_build\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: ./Dockerfile\n          platforms: linux/amd64\n          push: ${{ github.event_name != 'pull_request' }}\n          tags: ${{ steps.prep.outputs.tags }}\n          labels: |\n            org.opencontainers.image.title=${{ github.event.repository.name }}\n            org.opencontainers.image.description=${{ github.event.repository.description }}\n            org.opencontainers.image.url=${{ github.event.repository.html_url }}\n            org.opencontainers.image.source=${{ github.event.repository.clone_url }}\n            org.opencontainers.image.version=${{ steps.prep.outputs.version }}\n            org.opencontainers.image.created=${{ steps.prep.outputs.created }}\n            org.opencontainers.image.revision=${{ github.sha }}\n            org.opencontainers.image.licenses=${{ github.event.repository.license.spdx_id }}\nThe workflow code has almost none external dependencies, given that the creation of the Docker image name and its tags are within this code, however, it needs a pair of Secrets to log in at Docker Hub, in this case, the username used by Docker, and a Token to log in at Docker Hub (which can be generated here). With the username and Token, the user just needs to go at their repository, in the Settings tab and add the token in the Secrets subpage, as seen in the image below:\n\nWith these steps, the project should be able to be executed using GitHub Actions to allow the automation of the build, test, and deploy processes.\nIn the example used by this article, the final image can be found at Docker Hub here, and tested locally by running the command:\ndocker run -p 3838:3838 lhzpaese/ghactions_docker_shiny:latest\n\n\nSources\nMany of the references to projects and tools used in this article are added in form of Hyperlinks along with the text. However, I’d like to leave the Docker Webinar that served as inspiration for this project, presented by metcalfc, where he introduces the tools used for the integration of GitHub Actions and Docker.\n\nThanks for your attention, and any questions or suggestions, please do not hesitate to contact me at LinkedIn, and paeselhz."
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html",
    "href": "posts/rstudio-project-templates/index.html",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "",
    "text": "Many works in the data science realm rely solely upon the data scientist/analyst to guarantee the project standardization and code reproducibility; unfortunately, this becomes a source of confusion/disorganization since everybody (even on the same team) have different work strategies. To help ensure that teams share the same project standards, RStudio offers a sort of cookiecutter, where you can develop (in form of packages) many templates to be shared amongst users on the same team."
  }
]