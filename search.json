[
  {
    "objectID": "navbar/research.html",
    "href": "navbar/research.html",
    "title": "Luis Paese",
    "section": "",
    "text": "Presentations\n\n\nUsing R and Docker in Data Science\n\nIn Portuguese\nIn English\n\nReticulate 101\n\nIn Portuguese\nIn English\n\n\n\n\nPublications\n\n\nEarly childhood home-based programmes and school violence: evidence from Brazil\n\nWith MV Wink Junior and FG Ribeiro\nDevelopment in Practice 32 (2), 133-143\n\nImpacts of grade configuration on Brazilian student outcomes\n\nWith MV Wink Junior and MC Griebeler\nRevista Brasileira de Economia 75, 91-115\n\nLate and Unequal: Measuring Enrolments and Retention in Brazilian Education, 1933-2010\n\nWith TH Kang and NFA Felix\nRevista de Historia Economica-Journal of Iberian and Latin American Economic History, 1-28\n\nThe diversification benefits of cryptocurrencies in multi-asset portfolios: Cross-country evidence\n\nWith JA Colombo, FIL Cruz and RX Cortes\nAvailable at SSRN 3776260\n\nInequality of educational opportunities: Evidence from Brazil\n\nWith MV Wink Junior\nEconomiA 20 (2), 109-120\n\nAre Cryptocurrencies Suitable for Portfolio Diversification? Cross-Country Evidence\n\nWith JA Colombo, FIL Cruz and RX Cortes\n47¬∫ ENCONTRO NACIONAL DE ECONOMIA - ANPEC\n\nBuilding State-Level Business Cycle Tracer Tools: Evidence from a Large Emerging Economy\n\nWith JA Colombo, FIL Cruz and RX Cortes\nInternational Journal of Economics and Finance 10 (5), 14-30\n\nN√≠vel e desigualdade de aprendizado escolar: uma an√°lise a partir dos desempenhos dos Coredes ga√∫chos no Sistema Nacional de Avalia√ß√£o da Educa√ß√£o da B√°sica 2013 - In Portuguese\n\nWith MV Wink Junior\nIndicadores Econ√¥micos FEE 44 (4), 43-52\n\n\n\n\nWorking Papers\n\n\nNo current Working Papers"
  },
  {
    "objectID": "navbar/about.html",
    "href": "navbar/about.html",
    "title": "Luis H. Z. Paese",
    "section": "",
    "text": "Hi there,\nI‚Äôm a Machine Learning Engineer, specializing in the development and deployment of machine learning artifacts, using software engineering best practices. Outside of my professional environments, I‚Äôm mostly interested in the development of open-source projects, as well as researching topics I find intriguing. This personal blog is a place where I convey some of the findings I come across during my research or my professional tasks as well as my takes on different topics that involve data science and software engineering.\n\n\n\n\nData Science\nMachine Learning\nSoftware Development\nCloud Architecture\n\n\n\n\n\nMachine Learning - Coursera\nGCP Cloud Architect Specialization - Coursera"
  },
  {
    "objectID": "posts/rstudio-emr/index.html",
    "href": "posts/rstudio-emr/index.html",
    "title": "RStudio Server in AWS EMR",
    "section": "",
    "text": "In a previous post that I‚Äôve written (which can be found here), I shared my views on the use of Infrastructure as Code tools, such as Packer and Terraform, to create Amazon Machine Images (AMI), creating a reproducible environment for data scientists and data analysts to explore Cloud resources with many tools pre-installed in the machine (Jupyterhub and RStudio Server).\nI shall not extend myself in this post, since it has already been covered, but the repository associated with the other blog post uses Ansible playbooks to install the required packages and libraries to develop analysis and models using both R and Python, however, the main reason that I‚Äôve explored the possibilities of creating custom AMIs is to use them as base images in the deployment of AWS EMR Clusters."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#the-reasoning",
    "href": "posts/rstudio-emr/index.html#the-reasoning",
    "title": "RStudio Server in AWS EMR",
    "section": "The reasoning",
    "text": "The reasoning\nMany people that have deployed EMR Clusters in the past, already know that the deployment of the cluster can be associated with bootstrap scripts that will provide the necessary tools that the Data Scientist, Analyst, or Engineer will use in their work, and even though this is a significant solution, it adds a time penalty to the deployment of the server (the more packages and libraries associated with the cluster initialization, the higher will be the time of deployment of the cluster).\nThe default deployment of EMR allows the provisioning of Jupyter Notebooks or Zeppelin notebooks along with the initialization of the cluster, however, R and RStudio users are left lagging behind since both solutions are not available natively in the EMR Cluster, so I felt compelled to explore other solutions that were both scalable and fast allowing every analyst/scientist/engineer to deploy their own cluster, do their work in what language best suits their needs, and then shut down the cluster when they are done.\nAfter some digging, I found out that after the release of EMR 5.7 (and later), the cluster could be deployed using a custom Amazon Machine image, only by following a few recommendations and best practices as suggested here, like using an Amazon Linux 2 for EMR releases greater than EMR 5.30 and 6.x. This was the trigger that led me to first, create a custom Amazon Machine Image using Packer and Ansible (to ensure the installation and configuration of an RStudio Server), and later on use this image to deploy an EMR Cluster, and do a few more configurations to ensure that the RStudio user would have access to the Spark environments, Hadoop and other features available by using an EMR Cluster."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#the-how-to",
    "href": "posts/rstudio-emr/index.html#the-how-to",
    "title": "RStudio Server in AWS EMR",
    "section": "The How-To",
    "text": "The How-To\nAs with my other blog post, this is associated with a repository that helps the user to:\n\nDeploy an EMR Cluster using Terraform and using a custom AMI\nConfigure the default RStudio Server user to access the Hadoop File System and Spark environments\nUse the AWS Glue Metastore along with the Spark cluster, so that data catalogs that already exist in Glue can be accessed by the EMR Cluster\n\nBeware that this deployment is AWS-focused, and I don‚Äôt have plans to develop this same project in other clouds right now, even though I know it is possible by using tools that are available from both Google Cloud Platform and Azure."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#deploying-the-emr-cluster-using-terraform",
    "href": "posts/rstudio-emr/index.html#deploying-the-emr-cluster-using-terraform",
    "title": "RStudio Server in AWS EMR",
    "section": "Deploying the EMR Cluster using Terraform",
    "text": "Deploying the EMR Cluster using Terraform\nIn the project repository, we have a stack to deploy the EMR cluster using Terraform scripts that are modularized, so any adaptations necessary to suit your infrastructure needs (such as the creation of keys, or permission of roles) can be implemented separately. As of today, Terraform expects the variables as described below, to allow the creation of the cluster master server and core nodes along with it.\nThis implementation also supports the use of spot instances in the master and core nodes, however, in any production environment, it is recommended that at least the master node be deployed as an On-Demand instance, since, if it shuts down, the whole cluster would be terminated.\nThe variables available in my Terraform configuration are as below:\n# EMR general configurations\nname = \"\" # Name of the EMR Cluster\nregion = \"us-east-1\" # Region of the cluster, must be the same region that the AMI was built\nkey_name = \"\" # The name of the key pair that can be used to SSH into the cluster\ningress_cidr_blocks = \"\" # Your IP address to connect to the cluster\nrelease_label = \"emr-6.1.0\" # The release of EMR of your choice\napplications = [\"Hadoop\", \"Spark\", \"Hive\"] # The applications to be available as the cluster starts up\n\n# Master node configurations\nmaster_instance_type = \"m5.xlarge\" # EC2 instance type of the master node The underlying architecture of the machine must be compatible with the one used to build the custom AMI\nmaster_ebs_size = \"50\" # Size in GiB of the EBS disk allocated to master instance\nmaster_ami = \"\"  # ID of the AMI created with the custom installation of R and RStudio\n# If the user chooses to set a bid price, it will implicitly create a SPOT Request\n# If left empty, it will default to On-Demand instancesmaster_bid_price = \"\"\n\n# Core nodes configurations\ncore_instance_type = \"m5.xlarge\" # EC2 instance type of each core node The underlying architecture of the machine must be compatible with the one used to build the custom AMI\ncore_ebs_size = \"50\" # Size in GiB of the EBS disk allocated to each core instance\ncore_instance_count = 1 # Number of core instances that the cluster can scale\n# If the user chooses to set a bid price, it will implicitly create a SPOT Request\n# If left empty, it will default to On-Demand instancescore_bid_price = \"0.10\"\nThese configurations should be enough to get you up and running with a custom EMR cluster."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#allowing-custom-ports-in-aws-emr",
    "href": "posts/rstudio-emr/index.html#allowing-custom-ports-in-aws-emr",
    "title": "RStudio Server in AWS EMR",
    "section": "Allowing custom ports in AWS EMR",
    "text": "Allowing custom ports in AWS EMR\nA few changes to enhance security were added since December 2020, that require the user to manually allow ports to be publically available as can be found here. If this is not your case, you can skip to the next session, but, if you desire to expose your RStudio Server instance to be publically accessible through the internet, you have to follow the steps described below:\n\nGo into the AWS Console > EMR\nSelect Block Public Access (which should be activated)\nEdit the port interval to allow port 8787 to be used publically by RStudio Server\n\nIf your desire is to deploy a custom server that will be available only to a few IP addresses, you can skip this step and set the ingress_cidr_blocks to your personal IP address."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#configuring-the-rstudio-server-to-use-the-full-extent-of-aws-emr-resources",
    "href": "posts/rstudio-emr/index.html#configuring-the-rstudio-server-to-use-the-full-extent-of-aws-emr-resources",
    "title": "RStudio Server in AWS EMR",
    "section": "Configuring the RStudio Server to use the full extent of AWS EMR resources",
    "text": "Configuring the RStudio Server to use the full extent of AWS EMR resources\nThe steps described below are already embedded in the Terraform scripts that deploy the cluster and are here merely for informational purposes if someone wishes to tweak with it and customize it.\nAfter the deployment of the server is added a further step that configures the environment variables that make it easier for RStudio to find the necessary files to run smoothly with Spark, Hive, Hadoop, and other tools available in the cluster. Also, this step adds RStudio users to the Hadoop group and allows it to modify the HDFS.\nThis step can be found here in the project."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#aws-glue-metastore-as-json-configuration-in-terraform-module",
    "href": "posts/rstudio-emr/index.html#aws-glue-metastore-as-json-configuration-in-terraform-module",
    "title": "RStudio Server in AWS EMR",
    "section": "AWS Glue metastore as JSON configuration in Terraform module",
    "text": "AWS Glue metastore as JSON configuration in Terraform module\nWith the rising of off-the-shelf tools, such as AWS Glue, that allow the transformation of data and storing both the data and its metadata in an easy way, there was a need to integrate the EMR Cluster with this stack. This is also already been done in the Terraform stack, so if the user desire to use a Hive Metastore instead of a managed AWS Glue Metastore, this part of the code needs to be removed.\nThis step is done by a configurations_json inside the EMR main Terraform, which receives multiple inputs about how the cluster should be configured, changing spark-defaults (allowing different SQL catalog implementations for example), hive-site configurations to use the AWS Glue Metastore as default hive-site, and so on. The list of possibilities to be configured can be found here."
  },
  {
    "objectID": "posts/rstudio-emr/index.html#final-remarks",
    "href": "posts/rstudio-emr/index.html#final-remarks",
    "title": "RStudio Server in AWS EMR",
    "section": "Final remarks",
    "text": "Final remarks\nThe deployment of this project aims to close the gap between the provisioning of Data Science related stacks, trying to ensure that, no matter the tool that the Data Scientists/Analyst/Engineer aims to use, they will have the opportunity to use it. By the end of the deployment, one should be able to access, in less than 10 minutes, a fully functional EMR cluster with R and RStudio server installed and configured in the master node of the EMR.\nAny issues related to this project, including suggestions, can be added to the Github repository."
  },
  {
    "objectID": "posts/immigrant-characteristics-canada/documentation/literary_review.html",
    "href": "posts/immigrant-characteristics-canada/documentation/literary_review.html",
    "title": "Luis Paese",
    "section": "",
    "text": "Literary Review - Paper for ICPP (2023)\n\n\nDavid Card and Giovanni Peri\n\nImmigration Economics: A Review\narticle\nCriticizes a book from George Borjas (Immigration economics, from now on, referred as IE)\nSays the contribution of the book is one-sided, with little to no attention to some of the benefits proposed by immigration in the host country market\nGeorge Borjas is the leading economic scholar of immigration\nBook is of graduate level\nIE focuses almost exclusively on the labor market, specifically understanding the determinantes of immigrants earnings\nFinds significant biases employed in the economic models that were developed from the correlations, thus, making the bias larger as larger is the initial share of immigrantes in the labor market\n\nDavid Card\n\nImpact of the Mariel Boatlift on the Miami Labor Market\narticle\nMariel immigrants increased the Miami labor force by 7%\nMariel influx appears to have had virtually no effect on the wages or unemployment rates of less-skilled workers\n\nMette Foged, Linea Hasager and Giovanni Peri\n\ncomparing the Effects of Policies for the Labor Market Integration of Refugees\narticle\n\nMichel Beine, Giovanni Peri and Morgan Raux\n\nInternational College Students‚Äô Impact on the US Skilled Labor Supply\narticle\n\nFrederic Docquier, Caglar Ozden and Giovanni Peri\n\nThe Labour Market effects on immigration and emigration in OECV countries\narticle\n\nFrancesco D‚ÄôAmuri and Giovanni Peri\n\nImmigration, Jobs and Employment Protection: Evidence from Europe before and during the Great Recession\narticle\n\nGiovanni Peri\n\nDo Immigrant workers depress the wages of native workers?\narticle\nDespite the common sense, no evidence was found to support that immigrant workers depress wages of natives\nMostly, native workers‚Äô wages have been insulated by differences in skills, specialization\nThere‚Äôs some evidence of a negative effect of newly settled immigrants on the wages of earlier immigrants\nPositive wage effects are weaker in countries with rigid labor markets\n\nMichael Baker and Dwayne Benjamin\n\nThe Performance of Immigrants in the Canadian Labor Market\nClosed Article\n\nYao Lu and Feng Hou\n\nImmigration System, Labor Market Structures and Overeducation of High Skilled Immigrants in the US and Canada\nClosed Article"
  },
  {
    "objectID": "posts/immigrant-characteristics-canada/index.html#introduction",
    "href": "posts/immigrant-characteristics-canada/index.html#introduction",
    "title": "The evolution of immigrant characteristics in Canada",
    "section": "Introduction",
    "text": "Introduction\nThroughout time, Canada has proven itself to be a receptive and supportive country of immigration, having its first immigration policies dating back to 1869, with the First Immigration Act. However, neither the immigration process stayed the same throughout the years, nor the population of immigrants that decide to choose Canada as their destination, and so, to accommodate this evolutionary process, changes were made to keep Canada as an attractive destination for immigrants from all over the world.\nTo try to better understand the evolution of the immigrant profiles, data released by Statistics Canada were used, more specifically, the Census of the Population. To perform this analysis, the Public Use Microdata File (PUMF) for the years 2001, 2006, 2011 and 2016 was used.\n\nTo account for differences in the release of the Census files through the years, it was kept only data for the population of 15 years and beyond, since in 2001 and 2006 there is no information about children under 15 years old.\n\nBefore diving into the analysis itself, it is necessary to establish a couple of concepts that will be used from now on. One of the more important ones is related to the generational status of Canadian residents interviewed by the Census. In this we can break down the available statuses into three main groups:\n\n\nFirst Generation: People born in countries other than Canada;\n\nSecond Generation: People born in Canada with at least one parent born in countries other than Canada;\nand Third Generation and beyond: People with both parents born in Canada;\n\nThese three characteristics of the population will be the stepping stone to the development of the following analysis since they grant us the possibility of understanding how the characteristics of immigrants evolve with time but also grant us an understanding of how the families that immigrants choose to create in Canada help shape the ever-changing Canadian demographic landscape.\nThe present essay starts by analyzing in raw numbers how the Canadian population evolved from 2001 until 2016, by splitting the population between the generational and immigration statuses stated before. As can be seen in Figure¬†1, the Canadian population has grown steadily since 2001, especially between 2001 and 2011, when growth stumbled and was majorly driven by the increase of the First Generation migrants‚Äô share of the population.\n\n\n\n\nFigure¬†1: Evolution of the Canadian Population based on data from the Census of the Population\n\n\n\n\nNonetheless, the raw data does not give a clear insight into how the decrease of the Third Generation was superseded by an increase of the First Generation migrants in Canada. This can be better understood in Figure¬†2, where the clear decrease in the share of the population that belongs to the Third Generation or beyond was accompanied by an almost equal growth of the share of the population taken by the First Generation.\n\n\n\n\nFigure¬†2: Evolution of the share Canadian Population by Generational Status"
  },
  {
    "objectID": "posts/immigrant-characteristics-canada/index.html#demographic-impact",
    "href": "posts/immigrant-characteristics-canada/index.html#demographic-impact",
    "title": "The evolution of immigrant characteristics in Canada",
    "section": "Demographic Impact",
    "text": "Demographic Impact\nAs can be seen more prominently in most recent years, immigration in Canada has been a vector for population renewal to the Canadian demographic scenario, with the Second Generation Canadians continuing with steady numbers throughout the years, and Third Generation Canadians declining. The distribution of ages of the people that decide to immigrate is not equally distributed, however, since many immigrants decide upon moving to another country after they reach a certain age.\nOne interesting way to analyze this data is by creating a measure between the shares of people born in countries other than Canada, and people born in Canada. We can define this measure in Equation¬†1, with \\(i\\) representing the age group that each person belongs, \\(t\\) representing the Census Year, \\(BornCanada\\) being the count of the population that was born in Canada, and \\(BornAbroad\\) representing the First Generation migrants, people that were born abroad.\n\\[\nDiffShare_{it} = \\frac{BornCanada_{it}}{BornCanada_{it} + BornAbroad_{it}} - \\frac{BornAbroad_{it}}{BornCanada_{it} + BornAbroad_{it}}\n\\tag{1}\\]\nBy clarifying what defines the differences in the share of the population, we can visualize in Figure¬†3 how different age groups have different proportions of people born abroad, and how it evolved during the years, with most of the groups showing an increase in the share of the population that is composed by First Generation migrants.\n\n\n\n\nFigure¬†3: Difference of share of population by Age Group and Census Year\n\n\n\n\nEven so, we can move further in the analysis, trying to encompass the differences between the earliest and latest data that we have, 2001 and 2016 respectively. In Figure¬†4 we can see that throughout the years there was an increase in the proportion of the population occupied by First Generation migrants, especially between the ages of 25 and 49 years old.\n\nIt is to be seen whether these First Generation migrants to Canada will decide on staying in the country, or going back to their countries of birth after they reach a certain age, if the earlier comes true, we should expect an increase in the number of residents in Canada with the ages of 70 years and more, that were born in other countries.\n\n\n\n\n\nFigure¬†4: Difference of share of population by Age Group for 2001 and 2016\n\n\n\n\nAnother interesting analysis that can be done on this data is related to the evolution of the Canadian age pyramid. Since the Third Generation and Beyond is decreasing in Canada, and the Second Generation is kept at a steady level, it is expected a movement toward the aging of the population. This phenomenon, however, is overridden by the increase of First Generation residents, keeping the economically active population growing as can be seen in Figure¬†5.\n\n\n\n\nFigure¬†5: Canadian Age Pyramid for the years 2001, 2006, 2011 and 2016\n\n\n\n\nThese changes in the Canadian demographic profile are impressive when analyzed at the country level, and they become even more impressive when we dive into provincial data, as we start seeing patterns of evolution for provinces that were historically more sought after by immigrants. By checking Figure¬†6, Figure¬†7, Figure¬†8 and Figure¬†9 in the plots below, we can see how large provinces such as Ontario, British Columbia, Alberta, and Manitoba can account for a large part of their growth to First Generation migrants, meanwhile, provinces like New Brunswick, Newfoundland and Labrador, Nova Scotia, and Prince Edward Island started showing patterns of population aging.\n\n\n2001\n2006\n2011\n2016\n\n\n\n\n\n\n\nFigure¬†6: Canadian Age Pyramid for each province in 2001\n\n\n\n\n\n\n\n\n\n\nFigure¬†7: Canadian Age Pyramid for each province in 2006\n\n\n\n\n\n\n\n\n\n\nFigure¬†8: Canadian Age Pyramid for each province in 2011\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Canadian Age Pyramid for each province in 2016"
  },
  {
    "objectID": "posts/immigrant-characteristics-canada/index.html#immigrant-profile-changes",
    "href": "posts/immigrant-characteristics-canada/index.html#immigrant-profile-changes",
    "title": "The evolution of immigrant characteristics in Canada",
    "section": "Immigrant profile changes",
    "text": "Immigrant profile changes\nUntil this point, we have been assessing the importance of immigration to Canada‚Äôs growth in terms of population, and it is undeniable that immigration played a significant role in other fields where Canada thrived. Having said that, not only the characteristics of Canada changed with time, but the characteristics of the migrants changed as well. To perform this assessment, I‚Äôve chosen two points of view to evaluate the evolution of immigrants in Canada: Education and Income.\nEducation\nThe transformations in the education levels of First Generation migrants move toward the increase of opportunities for highly skilled workers, most of them out-pacing their Canadian counterparts, by immigrating with higher levels of education, on average, when compared to their Canadian counterparts.\n\nTo assess the educational level of the population I chose to create an indicator of whether the highest education achieved by an individual was at least the equivalent of an Advanced Diploma (three years of a Bachelor‚Äôs Degree).\n\nFigure¬†10 shows clearly that there was a general movement in search of higher specializations by the general population, however, the rate of First Generation immigrants that arrive in Canada with post-secondary education has always been greater than the rate of the population with higher education for the other two generational statuses.\n\n\n\n\nFigure¬†10: Evolution of Education Levels by Generational Status\n\n\n\n\nIncome\nOn the other hand, even with a larger account of the population arriving with higher educational levels than their Canadian counterparts, this is not reflected in the annual income that First Generation migrants receive.\nFigure¬†11 allows us to take a glimpse at two interesting takes regarding income for the Canadian population. The first insight that is taken is the general growth of the income from 2001 to 2016, showing that even when we account for the inflation of the period, there is still a rise in the annual income for the entire population. The second insight that comes to mind is the fact that people that belong to the Second Generation status were always able to surpass the mean income that is received by both First Generation and Third Generation and beyond.\n\nFor income the values were kept between 1,000 CAD and 2,000,000 CAD\n\n\n\n\n\nFigure¬†11: Evolution of Mean Income by Generational Status\n\n\n\n\nWith all that being said, wages and income are generally a more sensitive topic that requires a more in-depth analysis of what might be the causes that create these discrepancies in income, as well as creating further comparisons using a more comprehensive set of variables that will be able to explain more accurately how this groups truly differ between themselves."
  },
  {
    "objectID": "posts/immigrant-characteristics-canada/index.html#final-remarks",
    "href": "posts/immigrant-characteristics-canada/index.html#final-remarks",
    "title": "The evolution of immigrant characteristics in Canada",
    "section": "Final Remarks",
    "text": "Final Remarks\nBy no means a short essay will be able to capture all of the nuances and intricacies that surround a complex immigration system and the economic and demographic scenarios of a country as big as Canada. However, by analyzing data released by Statistics Canada, this aims to shed a light on the important role that immigration plays in Canada, by being a vector for population renewal, a constant source of talent, and a milestone for diversity and inclusion for everybody that seeks shelter in the Great White North üá®üá¶."
  },
  {
    "objectID": "posts/gcp-rstudio-shiny-server/index.html",
    "href": "posts/gcp-rstudio-shiny-server/index.html",
    "title": "Google Cloud Platform - How-to deploy Shiny Server and RStudio Server",
    "section": "",
    "text": "Disclaimer: This guide was written before the dissemination of the use of containers in Cloud Environments and I maintained it only until 2018, after that period, it became much easier to deploy this applications using containers. However, this guide is still being used by people that desire to deploy their own applications of RStudio Server Open Source and Shiny Server Open Source, without the use of containers.\nThe original post can be found here."
  },
  {
    "objectID": "posts/gcp-rstudio-shiny-server/index.html#creating-a-vm-instance",
    "href": "posts/gcp-rstudio-shiny-server/index.html#creating-a-vm-instance",
    "title": "Google Cloud Platform - How-to deploy Shiny Server and RStudio Server",
    "section": "Creating a VM Instance",
    "text": "Creating a VM Instance\nSince the installation of the desired software is complete, now you need to create your first VM instance. At the side panel of Google Cloud Platform‚Äôs Console, you‚Äôll find the Compute Engine menu, and inside that, the VM Instances Option. Clicking on that you‚Äôll be prompted to create or import a Virtual Machine. Select Create, and you‚Äôll be taken to the following page:\n\n\n\nVM Instance Creation\n\n\nAt this section, you can choose a name for your virtual machine, later you can select where that VM instance will be hosted, the cheaper places are in the US, but given the distance, you might find the connection ‚Äúlaggy‚Äù. After picking the name and the zone, select what kind of instance you‚Äôll be holding. This will be given by your use of technologies, if you‚Äôll need lots of memory or lots of processing cores, here‚Äôs where you‚Äôll choose it. The price per month will depend on this configurations.\nAfter choosing the kind of horsepower that will equip your virtual machine, you can choose what kind of Operational System will come installed with it. For this project, I will choose Ubuntu 18.04, the newest LTS version of this OS available. Also, while choosing the OS, you can choose what kind of storage you‚Äôll need. For this project, we will choose 20 GBs hosted at an SSD, this choice can be made given the amount of space you need, for simple projects 20GB should be more than enough. This gives us speed and reliability.\nLater, the last thing to do is allow HTTP traffic and create the VM instance. This process might take a while. If all went correctly you probably will see something like this:\n\n\n\nCreated VM"
  },
  {
    "objectID": "posts/gcp-rstudio-shiny-server/index.html#setting-up-vm-connections-before-installing-rstudio-or-shiny-server",
    "href": "posts/gcp-rstudio-shiny-server/index.html#setting-up-vm-connections-before-installing-rstudio-or-shiny-server",
    "title": "Google Cloud Platform - How-to deploy Shiny Server and RStudio Server",
    "section": "Setting up VM connections before installing RStudio or Shiny Server",
    "text": "Setting up VM connections before installing RStudio or Shiny Server\nNow, you have a Virtual Machine hosted at Google Cloud Platform! At the external IP that is given to you, you can access different ports on that server, hosted at the cloud. But before we start installing R, Rstudio Server and Shiny Server, we need to create a secure connection between your local machine and the hosted Virtual Machine.\nTo do that you click on the arrow, at the left of SSH button, as shown below, and select ‚ÄúView gcloud Command‚Äù:\n\n\n\nVM SSH authentication\n\n\nThis step will generate a line of code that will be used to create a secure connection between your local machine, and your server. Do not share this snippet with anyone, since it can be used to connect to your server. Copy this line of code, and paste at the Google SDK app that we downloaded before.\nIt‚Äôll prompt you to authenticate your connection with Google, before downloading the private key. After that is done, you‚Äôll be able to find the private key at your Users folder, inside the /.ssh folder. If the authentication went correctly, the gcloud SDK will open a Putty session, running the Linux distro that is hosted by the virtual machine, you can close this since we‚Äôll authenticate our connection through WinSCP.\nAfter that is done, you‚Äôll open WinSCP, and create a new connection. At the Host you‚Äôll insert the EXTERNAL IP ADDRESS that is given by Google to your server, the door will remain 22, and the user will be the name of your Google account user. After filling this parts, leave the password blank, and click on the Advanced‚Ä¶ box, right below the password field. Inside of the Advanced Settings, go to SSH > Authentication at the map to your left. At the field that asks for your Private Key File, you‚Äôll click browse, and search for the \"google_compute_engine.ppk\" file that is stored in your User/.ssh/ folder.\nIf this setting went successfully, you‚Äôll be able to see at the left portion of your screen the files of your local machine and at the right side of your monitor, the files of your virtual machine. Success, we have managed to enter the files at the Google cloud server!\n\nAllowing ports for Rstudio Server and Shiny Server\nEven though we haven‚Äôt installed yet the RStudio nor the Shiny Server, we can allow the connection ports of the server that will be used later by this applications to connect via browser to our server.\nIn order to do that, you‚Äôll open the GCloud SDK feature, and paste the following codes: - For the RStudio Connection:\nsudo gcloud compute firewall-rules create rstudio-conn --allow=tcp:8787\n\nFor the Shiny Server Connection:\n\nsudo gcloud compute firewall-rules create shiny-conn --allow=tcp:3838\nDone! Now you‚Äôll be able to access the hosted process by RStudio and Shiny Server"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nThe evolution of immigrant characteristics in Canada\n\n\nAnalyzing Census of Population data to assess the immigrant‚Äôs profile changes\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\nRStudio Server in AWS EMR\n\n\nEasy deployment, reproducible, and fast\n\n\n\nMar 31, 2021\n\n\n\n\n\n\n\n\n\n\n\nData Science meets Infrastructure as Code\n\n\n\nMar 17, 2021\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub Actions to speed up CI/CD in data science projects\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio Project Templates to help the project standardization in data science teams\n\n\nImproving team collaboration and project standardization in data science\n\n\n\nApr 29, 2020\n\n\n\n\n\n\n\n\n\n\n\nGoogle Cloud Platform - How-to deploy Shiny Server and RStudio Server\n\n\nGuide to deploying RStudio Open Source and Shiny Server Open Source at GCP\n\n\n\nAug 24, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html",
    "href": "posts/rstudio-project-templates/index.html",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "",
    "text": "Many works in the data science realm rely solely upon the data scientist/analyst to guarantee the project standardization and code reproducibility; unfortunately, this becomes a source of confusion/disorganization since everybody (even on the same team) have different work strategies. To help ensure that teams share the same project standards, RStudio offers a sort of cookiecutter, where you can develop (in form of packages) many templates to be shared amongst users on the same team."
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#the-story-behind-project-templates",
    "href": "posts/rstudio-project-templates/index.html#the-story-behind-project-templates",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "The story behind project templates",
    "text": "The story behind project templates\nRecent studies aim to provide the society with methodologies that help users to better understand and organize their projects, data, and insights generated from this data. From CRISP-DM going all the way to Team Data Science Process, passing through KDD, we have experienced many ways to enhance our ability to work in teams, joining efforts to reach the desired insights faster, and with more reproducibility.\nNonetheless, this process repeats itself for an infinite number of times, and the process of going back and re-developing an insight, or re-training a model, becomes a challenge because the code that was written long ago, is not easily reachable, and may be hard to understand the whole process that took to develop this project a few weeks/months/years ago.\nA few of the projects may look like this:\n\n\nAn image representing an unorganized way to store data and code\n\n\nEven though it may be faster to just open a repository, and just put the scripts, files, functions, .gitignores and any sort of the files needed to deliver a shiny webpage or a machine learning model, or an analysis of some kind, this process lacks the ability to create code and processes that can be understood by any other person that may be assigned to this project in the future.\nSo, this RStudio Project Templates aims to help users create some sort of pattern when starting a project, helping the team to standardize the project organization and in a way that everybody can easily be assigned to a project, and understand what is happening there.\nAnd maybe, turn projects into something like this:\n-- project_directory/\n | -- data/\n    | -- raw/\n    | -- reference/\n    | -- processed/\n | -- scripts/\n    | -- modelling/\n    | -- analysis/\n    | -- production/\n | -- markdown/\n -- .gitignore\n -- project_directory.Rproj"
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#creating-a-new-r-package",
    "href": "posts/rstudio-project-templates/index.html#creating-a-new-r-package",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "Creating a new R Package",
    "text": "Creating a new R Package\nProject templates can be used to create new projects with a pre-specified structure, and one way to create these project templates is by creating an R package, which will allow the user to share its template with as many users as one desires.\nThe process of R package creation (a fairly simple one) is pretty straightforward. Hadley‚Äôs R Package Guide covers many of the caveats involved in the process of creating a package. But since our package will contain only one function, it shouldn‚Äôt require the complexity involved in the development of a more robust package.\nCreating an R package from the project‚Äôs menu is easy, and should leave the user in front of a sample package, which will contain the R package structure, a hello.R function, and a few other files. The first step is to remove the hello.R file located at R/ and the hello.Rd file located at man/. After that, we start with a clean package, and the first step is to create our function."
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#creating-a-function-that-will-create-a-template",
    "href": "posts/rstudio-project-templates/index.html#creating-a-function-that-will-create-a-template",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "Creating a function that will create a template",
    "text": "Creating a function that will create a template\nTo ‚Äúcreate the template‚Äù we have to instruct how R will deal with it, and how it will behave when we source this function in the New Project menu. So this function takes one mandatory argument, and the other is additional arguments that can help the logic of your Project Creation tool. The first argument will always be the path of this New Project since it‚Äôll create a .RProj file as well, it‚Äôll be hosted in a new folder. The other arguments of this function are passed as ... and they can be called within the code with their names, or by assigning them to dots <- list(...).\nOur function will do a few tasks, to display a few of the features that can be embedded in the project creation templates. It will:\n\nCreate a README.md file with writeLines();\nIf a checkbox is selected, it will create a .gitignore file;\nCreate a folder with a specific name given a selected input from the user;\n\nTo create a function, and ensure that roxygen2 can interpret it, and export the function that should be exported by the final package, we‚Äôll write the function according to the syntax below.\n\n#' This package will create a function called create_project()\n#'\n#' It's callback is at: inst/rstudio/templates/project/create_project.dcf\n#'\n#' @export\ncreate_project <-\nfunction(path, ...) {\n# Create the project path given the name chosen by the user:\ndir.create(path, recursive = TRUE, showWarnings = FALSE)\n# Change the working directory to the recently created folder:\nsetwd(file.path(getwd(), path))\n# Collect the list of inputs in a list to be called later:\ndots <- list(...)\n# In the project template we've added 2 choices for the user:\n# * One allows them to select if the project will have a .gitignore file\n# * The other will create a folder, given a select input from the user\n# Check .gitignore argument\nif(dots[[\"createGitignore\"]]) {\ngit_ignores <-\nc(\n'.Rhistory',\n'.Rapp.history',\n'.RData',\n'.Ruserdata',\n'.Rproj.user/',\n'.Renviron'\n)\nwriteLines(paste(git_ignores, sep = '\\n'), '.gitignore')\n}\n# Check selected folder\nif(dots[[\"folder\"]] == \"Production\"){\ndir.create(\"production\", recursive = TRUE, showWarnings = FALSE)\n} else {\ndir.create(\"development\", recursive = TRUE, showWarnings = FALSE)\n}\n}"
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#creating-the-.dcf-file",
    "href": "posts/rstudio-project-templates/index.html#creating-the-.dcf-file",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "Creating the .dcf file",
    "text": "Creating the .dcf file\nWith the function file created, the next step is to create the .dcf file. This file is responsible to create the boxes which the user will interact with. Along with the path that the user will type, you can create as many checkboxes/text inputs/select inputs as you wish to grant customization to the end-user. For our project, we‚Äôll create a checkbox, so the user can decide whether to create the .gitignore file and a select input, so the user can define the scope of the project (development or production);\nThis solution (development or production) was just created for illustration purposes, it does not reflect any actual state of a project.\nThe .dcf file must be created within the package in the folder inst/rstudio/templates/project with the name of your choice, and it should follow the syntax below:\nBinding: create_project\nTitle: My First Project Template\nOpenFiles: README.md\n# In the project you can also add icons (the icon should be a PNG, smaller than 64kb\n# and in the inst/rstudio/templates folder\n# Icon: name_of_pic.png\n\nParameter: folder\nWidget: SelectInput\nLabel: Choose the scope of the project\nFields: Production, Development\nDefault: Production\nPosition: left\n\nParameter: createGitignore\nWidget: CheckboxInput\nLabel: Create .gitignore\nDefault: On\nPosition: right"
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#devtoolsdocument-devtoolsinstall",
    "href": "posts/rstudio-project-templates/index.html#devtoolsdocument-devtoolsinstall",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "devtools::document() + devtools::install()",
    "text": "devtools::document() + devtools::install()\nNow that our package structure is created, we can use the devtools::document() function to create its documentation pages. Even though this package is mainly developed to be an add-in for RStudio, it‚Äôs a good practice to document the package, before installing it in R, since it will search for any dependencies and packages that are needed for your project to work.\nBy running the devtools::install() function, within our package, we are going to install it and make it available in any other R Session from now on. This should be enough to make the add-in accessible by RStudio as a New Project Template.\nAfter that, your project template should be available in RStudio‚Äôs list of projects. If you want to know more about different features to tweak your project template and add more customizability, please check the RStudio Project Template page."
  },
  {
    "objectID": "posts/rstudio-project-templates/index.html#conclusion",
    "href": "posts/rstudio-project-templates/index.html#conclusion",
    "title": "Using RStudio Project Templates to help the project standardization in data science teams",
    "section": "Conclusion",
    "text": "Conclusion\nGiven the recent developments in data science and its integration with software engineering techniques, the need to standardize projects within teams comes handy to ensure that everybody can understand, and be understood by their pairs. This allows faster debugging of projects and mitigates the drawbacks of legacy code that is incomprehensible by people in the same teams. Of course, this feature alone will not help to document code, or standardize the way to write R scripts (please refer to The tidyverse style guide), but it is a tool that will help users to create patterns regarding project management.\nCheck out the GitHub page where the code used in this article is hosted, and if you wanna try out this example, clone the repository, or install it in R by running: devtools::install_github(‚Äòpaeselhz/rstudioProjectTemplate‚Äô)."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html",
    "href": "posts/gh-actions-data-science/index.html",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "",
    "text": "As the latest advances regarding cloud computing, it has become even more necessary for the implementation of tools that are, at the same time, scalable, and that ensure the reproducibility of the execution. Having this need in mind, a few tools arose to the job, such as Docker, that allow the creation of a ‚Äúrecipe‚Äù of the application, ensuring that different builds of the same application run equally.\nDifferently than a Virtual Machine (VM), that provides an infrastructure through Hypervisors and emulates processors and memory, Docker shares these resources throughout the containers, allowing the developer to focus less on the infrastructure and more on the development of the application. Nonetheless, the containerization of projects and applications mitigates the expression ‚ÄúIt Runs on My Machine‚Äù, given that it tries to ensure that, independently on the platform chosen by the developer, the Docker container executes always in the same way.\nGiven that the benefits of containerization go beyond the development of applications and can be useful in other fields, many data scientists began to use Docker to containerize their analysis, model training, dashboards, and APIs, both to make the delivery of projects easier (given that it reduces the possibility of bugs) and to ensure that the results found once, can always be reached again."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#cicd-continous-integration-and-continuous-deployment",
    "href": "posts/gh-actions-data-science/index.html#cicd-continous-integration-and-continuous-deployment",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "CI/CD ‚Äî Continous Integration and Continuous Deployment",
    "text": "CI/CD ‚Äî Continous Integration and Continuous Deployment\nEven though many insights and machine learning models generated by data scientists are valuable, they fail to add value to the business they are inserted when the projects are stuck in a personal machine that cannot be used by other people. Therefore, to ensure that any modifications are identified and its results are expanded to other teams, there is the process of Continous Integration and Continous Deployment (CI/CD), that allows the automation of the testing and deploy processes in initial versions of the project.\nMany people may be familiar with the concepts of CI/CD, however, many of the tools that are used for this process are paid (such as Jenkins, CircleCI, and TravisCI), limiting its use only to people that:\n\nare willing to pay the price of these tools;\nor are working in a company that already has a CI/CD cycle deployed."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#github-and-github-actions",
    "href": "posts/gh-actions-data-science/index.html#github-and-github-actions",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "GitHub and GitHub Actions",
    "text": "GitHub and GitHub Actions\nGitHub is a well-known code versioning platform that has more than 40 million users and more than 100 million repositories, being an enormous source of open-source code, available to thousands of people anywhere in the world.\nAiming to support the creation of open-source projects, as well as allow their users to abstract the processes involving CI/CD, GitHub created in 2019, the GitHub Actions tool. It allows the automation of workflows defined by the user to help with integrated tests, validation of Pull Requests, and many other features. Moreover, the number of actions to be used by the user grows by the day, given that many companies are looking to develop tools to help the users community. Many of these actions already allow the integration of many popular tools such as Docker, AWS CloudFormation, Terraform, and many others that can be found here.\nEven though GitHub Actions is free to use only for non-private repositories, there are different levels of use that can be leveraged in private projects, before considering the use of any GitHub Enterprise tools. This opens the doors so that many people that develop open-source projects can test their tools and spread their findings in a more automated and scalable way."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#docker-login-docker-build-and-push",
    "href": "posts/gh-actions-data-science/index.html#docker-login-docker-build-and-push",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "Docker Login & Docker Build and Push",
    "text": "Docker Login & Docker Build and Push\nOne of the tools developed to be used alongside GitHub Actions is the login actions in repositories that allow the storage of Docker images (such as Docker Hub, ECR from AWS, GCR from GCP), as well as the build of these images without the need to occupy the user‚Äôs machine. With these in mind, two actions are declared in the CI workflow file, that can be found in these links: docker/login-action and docker/build-push-action."
  },
  {
    "objectID": "posts/gh-actions-data-science/index.html#inserting-data-science-in-the-cicd-process",
    "href": "posts/gh-actions-data-science/index.html#inserting-data-science-in-the-cicd-process",
    "title": "Using GitHub Actions to speed up CI/CD in data science projects",
    "section": "Inserting Data Science in the CI/CD process",
    "text": "Inserting Data Science in the CI/CD process\nThe Data Science field is full of different frameworks, dependencies, and different languages that can be used according to the need and abilities of the data scientist, but a common truth amongst them is that they all have the possibility of being encapsulated by a containerization process, helping to ensure the reproducibility of the project.\nWith that in mind, the example used by me to deploy the automation tool of GitHub Actions involves the development of a web application using R‚Äôs Shiny library. Nevertheless, the same workflow implementation could be used to deploy APIs developed using Python‚Äôs FastAPI, i.e., or any other framework that can be encapsulated in a Docker container.\nThe project can be found here: paeselhz/ghActionsDockerShiny. I won‚Äôt be entering in details of the development of the application, because the example used by me is relatively simple and have no elaborate development. The focus of this article is the containerization of the project, and the workflow automation to build the image and store it in Docker Hub, making it available for further downloads.\n\nCreating the Dockerfile\nFor those familiarized with the Dockerfile and its syntax, the execution is the same as expected in a project that will be developed, built, and ran locally with Docker. In it, we declare the base image that will be used for further installation of libraries and dependencies, as well as the configuration of the project, file copy, and other steps that usually can be added to a Dockerfile.\nFROM rocker/shiny:4.0.0\n\nRUN apt-get update \\\n  && apt-get install -y \\\n    libxml2-dev \\\n    libglpk-dev \\\n  && install2.r \\\n    --error \\\n    dplyr \\\n    shiny \\\n    purrr \\\n    highcharter \\\n    shinyWidgets \\\n    shinycssloaders \\\n    devtools \\\n    xml2 \\\n    igraph \\\n    readr\n  \nRUN R -e \"devtools::install_github('wilsonfreitas/rbcb')\"\nCOPY . /srv/shiny-server\nRUN chmod -R 777 /srv/shiny-server\nThis script, which is located in the project root directory, is responsible to gather an image that already has Shiny and its dependencies installed, and the installation of libraries that will be used by the app developed within R.\n\n\nCreating the Workflow file\nTo GitHub Actions know which steps need to be taken for the workflow automation, it becomes necessary to create a file within the project that will be located at .github/workflows/main.yml, the file syntax is the same as any YAML file, being easy to code. In case the user does not want to do this process locally and commit the changes, GitHub itself has an online code editor for the creation of the workflow.\nIn this file are declared a few steps such as the name of the workflow, the triggers that will be used to deploy the workflow execution, and the jobs that it will be responsible for executing. The name and trigger parts of the file are highly customizable, and the user can change it in many ways, moreover, in the part of the job, there are a few steps that are needed for the job to login in Docker Hub, configure BuildX (a tool that will be used to build the image), configure QEMU (a tool that will be allowing multi-platform builds), deploy the built image to Docker Hub, logout and clean the machine to ensure that no processes are still running.\n# Setting up a Workflow to work with Github Actions\nname: ci\n# Controls to when trigger the GH Action\n# Below are configurations to the following triggers:\n# - commits at master branch\n# - tag commits at the project\n# - scheduled to run at 01:00GMT\n# The user can also configure triggers at pull requests\n# as well as remove branches from triggering GH Actions\non:\n  push:\n    branches: [ master ]\n    tags: [ '*.*.*' ]\n  schedule:\n    - cron: '0 1 * * *'\n# Below there is the job configuration to build the image\n# and push it to a DockerHub repository\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Checkout\n        uses: actions/checkout@v2\n      -\n        name: Prepare\n        id: prep\n        run: |\n          DOCKER_IMAGE=<USER_NAME>/<REPOSITORY_NAME>\n          VERSION=noop\n          if [ \"${{ github.event_name }}\" = \"schedule\" ]; then\n            VERSION=nightly\n          elif [[ $GITHUB_REF == refs/tags/* ]]; then\n            VERSION=${GITHUB_REF#refs/tags/}\n          elif [[ $GITHUB_REF == refs/heads/* ]]; then\n            VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r 's#/+#-#g')\n            if [ \"${{ github.event.repository.default_branch }}\" = \"$VERSION\" ]; then\n              VERSION=edge\n            fi\n          fi\n          TAGS=\"${DOCKER_IMAGE}:${VERSION}\"\n          if [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n            MINOR=${VERSION%.*}\n            MAJOR=${MINOR%.*}\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:${MINOR},${DOCKER_IMAGE}:${MAJOR},${DOCKER_IMAGE}:latest\"\n          elif [ \"${{ github.event_name }}\" = \"push\" ]; then\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:sha-${GITHUB_SHA::8}\"\n          fi\n          echo ::set-output name=version::${VERSION}\n          echo ::set-output name=tags::${TAGS}\n          echo ::set-output name=created::$(date -u +'%Y-%m-%dT%H:%M:%SZ')\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v1\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v1\n      -\n        name: Login to DockerHub\n        if: github.event_name != 'pull_request'\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        id: docker_build\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: ./Dockerfile\n          platforms: linux/amd64\n          push: ${{ github.event_name != 'pull_request' }}\n          tags: ${{ steps.prep.outputs.tags }}\n          labels: |\n            org.opencontainers.image.title=${{ github.event.repository.name }}\n            org.opencontainers.image.description=${{ github.event.repository.description }}\n            org.opencontainers.image.url=${{ github.event.repository.html_url }}\n            org.opencontainers.image.source=${{ github.event.repository.clone_url }}\n            org.opencontainers.image.version=${{ steps.prep.outputs.version }}\n            org.opencontainers.image.created=${{ steps.prep.outputs.created }}\n            org.opencontainers.image.revision=${{ github.sha }}\n            org.opencontainers.image.licenses=${{ github.event.repository.license.spdx_id }}\nThe workflow code has almost none external dependencies, given that the creation of the Docker image name and its tags are within this code, however, it needs a pair of Secrets to log in at Docker Hub, in this case, the username used by Docker, and a Token to log in at Docker Hub (which can be generated here). With the username and Token, the user just needs to go at their repository, in the Settings tab and add the token in the Secrets subpage, as seen in the image below:\n\nWith these steps, the project should be able to be executed using GitHub Actions to allow the automation of the build, test, and deploy processes.\nIn the example used by this article, the final image can be found at Docker Hub here, and tested locally by running the command:\ndocker run -p 3838:3838 lhzpaese/ghactions_docker_shiny:latest\n\n\nSources\nMany of the references to projects and tools used in this article are added in form of Hyperlinks along with the text. However, I‚Äôd like to leave the Docker Webinar that served as inspiration for this project, presented by metcalfc, where he introduces the tools used for the integration of GitHub Actions and Docker.\n\nThanks for your attention, and any questions or suggestions, please do not hesitate to contact me at LinkedIn, and paeselhz."
  },
  {
    "objectID": "posts/ds-iac/index.html",
    "href": "posts/ds-iac/index.html",
    "title": "Data Science meets Infrastructure as Code",
    "section": "",
    "text": "The recent developments towards the integration of Data Science features, using best practices of software engineering, brought knowledge of tools like Docker, Jenkins, REST APIs to many data scientists, and even though this knowledge enables most of them to deploy their data science projects adding value to the business they are inserted, there are a few caveats and compromises when a project is built locally and then is rethought to be deployed in the cloud.\nAware of these caveats, my main objective with this article is to merge both the knowledge of Data Science (and its tools) with Infrastructure as Code tools such as Packer and Terraform to ensure the creation of a data science environment that helps users that work in both R and Python to provision Infrastructure at the cloud, and leverage many Big Data tools, such as AWS EMR to analyze and process large amounts of data.\nEven though this article is created using an AWS stack (It may incur costs since the whole infrastructure goes beyond the AWS Free Tier), it can be used in other cloud providers with proper modifications to both the Packer image creation and the Terraform to deploy the machines. Further updates in this work should bring support for more clouds to the project."
  },
  {
    "objectID": "posts/ds-iac/index.html#tldr",
    "href": "posts/ds-iac/index.html#tldr",
    "title": "Data Science meets Infrastructure as Code",
    "section": "TL;DR",
    "text": "TL;DR\nThe main objective of this project (available here in Github) is to create a set of tools that allow the deployment of a data science stack using Packer and Terraform. It comprises the following infrastructure:\n\nThe creation of a Packer image with both JupyterHub hosted in port 8000, and RStudio Server hosted in port 8787, the result of this packer image will be an Amazon Machine Image that can be used multiple times in the future.\nCreation of a VPC and necessary subnets to ensure that all the infrastructure required by the stack will be available.\nFinally, when all of that is created, the objective will be to deploy both of these services (JupyterHub and RStudio Server) in an EC2 instance."
  },
  {
    "objectID": "posts/ds-iac/index.html#building-a-custom-image-to-be-used",
    "href": "posts/ds-iac/index.html#building-a-custom-image-to-be-used",
    "title": "Data Science meets Infrastructure as Code",
    "section": "Building a custom image to be used",
    "text": "Building a custom image to be used\nLeveraging the customization of an image by using Packer and creating custom Machine Images to be deployed in any cloud, we also use Ansible to create playbooks to allow the customization of what should be installed in the machine. By creating this image with a single source, and specifying many of the features added to the Machine Image, we can ensure that once the image is built, it will be the same throughout its lifetime, meaning that any packages and dependencies installed on the machine, should not be updated in its daily usage.\nThis allows users, being data scientists or data analysts, to be certain that once a project is built on top of a custom AMI, none of the dependencies will change with time. This reproducibility feature can also be enhanced, when the user itself uses dependencies managers such as Renv for R, or the requirements.txt in an environment when using Python.\nThe creation of the image can be customized to add or remove features, such as installing or not RStudio Server, or deploying the image with Anaconda installed or not, and after the AMI is finally built by Packer and Ansible, it will be available to be used multiple times.\nThe AMI that is being built by this project focus on the installation and configuration of an RStudio Server, and Jupyterhub with conda."
  },
  {
    "objectID": "posts/ds-iac/index.html#using-the-custom-image",
    "href": "posts/ds-iac/index.html#using-the-custom-image",
    "title": "Data Science meets Infrastructure as Code",
    "section": "Using the custom image",
    "text": "Using the custom image\nMany users, when starting to use Cloud Providers to deploy powerful machines to train models or analyze large amounts of data, do not use tools that allow the provision of this infrastructure in a manner that is both reproducible and scalable (and many times these resources end up amounting for a large cloud bill by the end of the month). This is when Terraform comes along since it helps users to deploy a custom infrastructure using code, and destroy the same infrastructure when the work is done.\nThe terraform module of this project contains a simplified version of infrastructure, but as well as with Packer, more modules can be added by the users if needed. The security module controls the ingress and egress ports used by the machine that will be deployed using a Custom AMI, while the network module controls the networks and gateways that will be used by the machine, the IAM sets the permissions that this machine can have in the cloud provider and the EC2 module is what actually deploys the machine.\nMany of the advantages brought by Terraform, and its orientation to Infrastructure as Code, allow for the tracking of infrastructure changes through time with Git. By the end of Terraform deployment, the user will have the IP address of the machine, so it can connect and use its resources (and applications previously installed by Packer), RStudio server being hosted in port 8787, and Jupyterhub in port 8000 (these ports were previously allowed in the security groups module)."
  },
  {
    "objectID": "posts/ds-iac/index.html#further-developments",
    "href": "posts/ds-iac/index.html#further-developments",
    "title": "Data Science meets Infrastructure as Code",
    "section": "Further developments",
    "text": "Further developments\nEven though this project can be used by itself as a Data Science stack that can easily be deployed to the cloud, we can also use this custom AMI with an EMR cluster, speeding up its provisioning, since we can take away a few installations that normally are executed during the bootstrap step of the cluster provisioning.\nAlso, this does not replace any formal and official releases of Data Science stacks that allow data exploration and model creation and deployments such as Sagemaker, Google AI Platform, or Azure Data Studio but is an alternative to these solutions since it allows the user to fully customize what they need and deploy it in whatever cloud they want."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Density of applicants in UFRGS entrance exam - In Portuguese\n\n\nHistorical data from 2016 until 2023\n\n\n\n\n\nThe purpose of this application is to allow for exam applicants to understand how disputed will be the entrance to specific courses at the Federal University of Rio Grande do Sul (UFRGS). This allows the user to view this data with different filters and to export this data to analyze themselves.\n\n\n\nLive App GitHub\n\n\n\n\n\n\n\n\nBrazilian Elections Data - In Portuguese\n\n\n2022 - 2nd term\n\n\n\n\n\nShiny application to follow the voting counts for the Brazilian 2022 presidential elections in the second term. This application sources live data from the TSE (Superior Electoral Court of Brazil), allowing for the user to check in real time the voting count. The application includes data in the country level, state level and municipality level.\n\n\n\nLive App GitHub\n\n\n\n\n\n\n\n\nContainerized Selenium API\n\n\nRunning a Docker Container with Fast API, and requesting data using selenium\n\n\n\n\nThe main objective of this repository is to deploy a Docker container with both, FastAPI and Selenium, to check the possibilities when we try to use FastAPI to receive parameters, and run a selenium process in the background to gather and parse data.\n\n\n\nGitHub\n\n\n\n\n\n\n\n\nINEP/MEC Microdata scripts - In Portuguese\n\n\nCollection of repositories related to ETL of educational microdata available in Brazil\n\n\n\n\nThe repositories contain scripts related to the process of importing, cleaning, and transforming microdata extracted from Brazil‚Äôs Ministry of Education. Both repositories assume that the user has already downloaded the datasets\n\n\nGitHub - Scholar Census GitHub - SAEB until 2005"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luis Paese",
    "section": "",
    "text": "Featured Projects\n\n\n\n\n\n\nDensity of candidates\n\n\nApplication to allow exam candidates to view the density of candidates for the entrance exam at the Federal University of Rio Grande do Sul.\n\nGitHub\n\n\n\n\n\n\n\nBrazilian 2022 Elections\n\n\nShiny application to view live voting count for the Brazilian 2022 presidential elections, second term.\n\nGitHub\n\n\n\n\n\n\n\nContainerized Selenium\n\n\nThis experiment allows the user to deploy a Selenium scraping inside a FastAPI deployment.\n\nGitHub\n\n\n\n\n See all projects\n\n\nFeatured Posts\n\n\n\n\n\n\n\nThe evolution of immigrant characteristics in Canada\n\n\nAnalyzing Census of Population data to assess the immigrant‚Äôs profile changes\n\n\n\n\n\n\nMar 1, 2023\n\n\nLuis HZ Paese\n\n\n25 min\n\n\n\n\n\n\n\n\nUsing GitHub Actions to speed up CI/CD in data science projects\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\nLuis HZ Paese\n\n\n11 min\n\n\n\n\n\n\n\n\nUsing RStudio Project Templates to help the project standardization in data science teams\n\n\nImproving team collaboration and project standardization in data science\n\n\n\n\n\n\nApr 29, 2020\n\n\nLuis HZ Paese\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n\n\n See all posts"
  },
  {
    "objectID": "recipes/index.html",
    "href": "recipes/index.html",
    "title": "Recipes",
    "section": "",
    "text": "Sometimes I enjoy taking off my ML Engineer hat and like putting on the cook hat. Below are some experiments and recipes that I‚Äôve done in the past.\n\nI try my best to develop the recipes in both English and Portuguese, however, any suggestions for changes are welcome.\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSalt bread/P√£o franc√™s\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recipes/salt-bread/index.html",
    "href": "recipes/salt-bread/index.html",
    "title": "Salt bread/P√£o franc√™s",
    "section": "",
    "text": "Para a vers√£o em portugu√™s, clique aqui."
  },
  {
    "objectID": "recipes/salt-bread/index.html#sponge",
    "href": "recipes/salt-bread/index.html#sponge",
    "title": "Salt bread/P√£o franc√™s",
    "section": "Sponge",
    "text": "Sponge\nThe role of a sponge in bread making is not to activate the yeast, but to give it the strength it will need to rise with the next additions of flour and other ingredients, this is why the process of creating the sponge feeds the yeast with water and flour.\n\nIngredients\n\n1/4 cup of flour\n1 tablespoon of dry yeast\n3 tablespoons of water\n\n\n\nSteps\n\nMix the flour and the yeast with a fork, and slowly start adding the water;\nStart mixing to create a homogeneous mixture;\nAfter mixing it up, leave it to rest for 1/2 hour to 1 hour (the time will depend on the temperature of the room, to allow the sponge to grow effectively);"
  },
  {
    "objectID": "recipes/salt-bread/index.html#dough",
    "href": "recipes/salt-bread/index.html#dough",
    "title": "Salt bread/P√£o franc√™s",
    "section": "Dough",
    "text": "Dough\n\nIngredients\n\n2 + 3/4 cups of flour\n1/2 tablespoon of salt\n1/2 tablespoon of sugar\n1 + 1/2 tablespoon of butter\n\nThe butter cannot be too cold, neither too hot, but it needs to be warm enough to be incorporated to the dough\n\n1 cup of water\n\n\n\nSteps\n\nStart mixing up the solid ingredients (flour, salt and sugar), add the sponge and the butter to the mixture;\nSlowly start adding water to control the dough humidity, the dough should not be to moist, neither too dry;\n\nThe amount of water that will be added to the mixture will vary depending on the water absorption of the flour\n\nKnead the dough for a time between 10 to 15 minutes, this time might vary, specially if you are doing it by hand, but it is important that the dough reaches a point when it can be stretched without tearing apart;\nLeave the dough to rest for 20~30 minutes covered with a wet towel;"
  },
  {
    "objectID": "recipes/salt-bread/index.html#shape-the-bread",
    "href": "recipes/salt-bread/index.html#shape-the-bread",
    "title": "Salt bread/P√£o franc√™s",
    "section": "Shape the bread",
    "text": "Shape the bread\nMost of the dough growing will take place after the bread is cast into shape, this is why the first fermenting step is quick. If the dough doesn‚Äôt grow as expected in this first 20 to 30 minutes, there is no worry, since the bread will grow more after casting.\n\nFirst Cast\n\nSplit the dough into 6 even parts, each of the parts will be shaped as a stretched diamond, thinner in the edges and thicker in the middle;\nTake one edge of the bread, and start rolling it into the dough, casting it into a cylindrical format, with 6 or 7 rolls;\nDo it for the 6 parts, and let them rest for another 20 to 30 minutes;\n\n\n\nSecond Cast\n\nTake each cylinder, and stretch it back again to the stretched diamond format;\nFrom the edge of the bread, start rolling it into the dough as in the previous step, however, rolling it tighter, to create tension in the outer layer of the bread, casting it into a cylindrical format again, with 9 to 10 rolls;\nDo it for the 6 breads, and leave it to grow for 1 hour."
  },
  {
    "objectID": "recipes/salt-bread/index.html#baking",
    "href": "recipes/salt-bread/index.html#baking",
    "title": "Salt bread/P√£o franc√™s",
    "section": "Baking",
    "text": "Baking\nTo bake this bread, the oven will need to have high humidity levels, to allow the bread to create a thick crust. One way that can be used to create humidity inside the oven is to place a tray with boiling water and a towel inside, allowing the water to evaporate inside the oven.\n\nSet the oven to 220¬∫C (430¬∫ F) and add the tray with boiling water;\nInsert the dough rolls in the oven;\nWhen it hits the 15 minute mark, remove the tray with boiling water;\nLeave the bread for another 10 to 15 minutes, which should be enough time to harden the bread crust and leave it with a golden color;\n\nHappy baking!"
  },
  {
    "objectID": "recipes/salt-bread/index-pt-br.html",
    "href": "recipes/salt-bread/index-pt-br.html",
    "title": "P√£o franc√™s/Salt bread",
    "section": "",
    "text": "For the english version, click here."
  },
  {
    "objectID": "recipes/salt-bread/index-pt-br.html#esponja",
    "href": "recipes/salt-bread/index-pt-br.html#esponja",
    "title": "P√£o franc√™s/Salt bread",
    "section": "Esponja",
    "text": "Esponja\nA fun√ß√£o da esponja n√£o √© ativar o fermento, mas sim dar for√ßa pra ele, por isso ela √© com um √°gua e farinha, tendo mais for√ßa de crescimento quando forem adicionados os demais ingredientes da receita.\n\nIngredientes\n\n1/4 de x√≠cara de farinha\n10g de fermento biol√≥gico seco\n3 colheres de sopa de √°gua\n\n\n\nPassos\n\nMisturar o fermento e a farinha em uma tigela, e aos poucos ir adicionando as colheres de √°gua;\nMexer bem at√© formar uma pasta homog√™nea;\nDeixar descansando por 30 a 60 minutos (o tempo de crescimento da esponja vai variar com a temperatura);"
  },
  {
    "objectID": "recipes/salt-bread/index-pt-br.html#massa-do-p√£o",
    "href": "recipes/salt-bread/index-pt-br.html#massa-do-p√£o",
    "title": "P√£o franc√™s/Salt bread",
    "section": "Massa do p√£o",
    "text": "Massa do p√£o\n\nIngredientes\n\n2 + 3/4 de x√≠cara de farinha\n1/2 colher de sopa de sal\n1/2 colher de sopa de acucar\n1 + 1/2 colher de sopa de manteiga\n\nA manteiga precisa estar em um ponto que seja liquido o suficiente para incorporar na massa, mas n√£o pode estar muito quente para n√£o afetar o crescimento da massa.\n\n1 x√≠cara de √°gua\n\n\n\nPassos\n\nMisturar primeiro os ingredientes secos (farinha, sal e a√ß√∫car), depois adiciona a manteiga e a esponja;\nAdicionar √°gua aos poucos para a massa n√£o ficar muito seca, nem muito √∫mida;\n\nA quantidade de √°gua varia por causa da capacidade de absor√ß√£o diferente de cada tipo de farinha;\n\nSovar a massa de 10 a 15 minutos, esse tempo pode variar, mas o importante √© que a massa chegue em um ponto que ela seja capaz de ser esticada sem rasgar;\nDeixar a massa em repouso por 20 a 30 minutos, coberta com um pano √∫mido para evitar que a massa resseque;"
  },
  {
    "objectID": "recipes/salt-bread/index-pt-br.html#moldar-a-massa",
    "href": "recipes/salt-bread/index-pt-br.html#moldar-a-massa",
    "title": "P√£o franc√™s/Salt bread",
    "section": "Moldar a massa",
    "text": "Moldar a massa\nO maior tempo de crescimento da massa vai acontecer depois que os p√£es estiverem no formato de p√£o franc√™s, por isso a primeira fermenta√ß√£o √© relativamente r√°pida. Se a massa n√£o crescer muito durante esses primeiros 20 a 30 minutos, n√£o √© problema, dado que o p√£o vai crescer mais quando estiver no formato final.\n\nPrimeiro Molde\n\nDivida a massa em 6 partes iguais, e molde cada uma das partes como um losango, com as pontas mais finas, e mais espesso no meio;\nPegue uma ponta do losango, e comece a enrolar, formando o formato do p√£o franc√™s, com 6 a 7 enroladas;\nFa√ßa isso para as 6 partes, e deixe em repouso por 20 a 30 minutos;\n\n\n\nSegundo Molde\n\nPegue cada p√£o franc√™s, e estique novamente a massa no formato de losango;\nPegue uma ponta do losando, e come√ße a enrolar, como foi feito no primeiro molde mas fa√ßa rolos mais ‚Äúapertados‚Äù, para criar tens√£o na superf√≠cie externa da massa, enrole de 9 a 10 vezes;\nFa√ßa isso para os 6 p√£es, e deixe em repouso por 1 hora;"
  },
  {
    "objectID": "recipes/salt-bread/index-pt-br.html#assar",
    "href": "recipes/salt-bread/index-pt-br.html#assar",
    "title": "P√£o franc√™s/Salt bread",
    "section": "Assar",
    "text": "Assar\nPara assar o p√£o franc√™s, o forno precisa ter uma quantidade alta de umidade, que √© o que vai permitir o p√£o criar a casquinha de p√£o franc√™s. Uma forma de aumentar a umidade do forno √© utilizar uma forma com √°gua fervendo e uma toalha dentro, permitindo que a √°gua evapore dentro do forno.\n\nColoque o forno em 220¬∫C (430¬∫ F) e adicione a forma com √°gua fervendo;\nInsira os p√£es franceses no forno;\nQuando atingir os primeiros 15 minutos, retire a forma com √°gua fervendo;\nDeixe os p√£oes por mais 10 a 15 minutos, tempo suficiente para a casca do p√£o endurecer e ficar com uma cor dourada;\n\nHappy baking!"
  }
]